{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "import PIL\n",
    "from tqdm.auto import tqdm\n",
    "from ultralytics import YOLO\n",
    "import copy\n",
    "from diffusers import StableDiffusionInpaintPipeline\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from skimage import feature\n",
    "import torchvision.transforms as T\n",
    "import io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = ''\n",
    "with open(f'{BASE_PATH}captions_train2017.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    data = data['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{BASE_PATH}captions_train2017.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    data = data['annotations']\n",
    "\n",
    "img_cap_pairs = []\n",
    "\n",
    "for sample in data:\n",
    "    img_name = '%012d.jpg' % sample['image_id']\n",
    "    img_cap_pairs.append([img_name, sample['caption']])\n",
    "\n",
    "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
    "captions['image'] = captions['image'].apply(\n",
    "    lambda x: f'{BASE_PATH}train2017/{x}'\n",
    ")\n",
    "captions = captions.sort_values(by=[\"image\"])\n",
    "captions = captions.drop_duplicates(subset=['image'], keep='first')\n",
    "captions = captions[:20000]\n",
    "captions = captions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vae/diffusion_pytorch_model.safetensors not found\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "/home/hakim/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\", 0)\n",
    "    \n",
    "pipe = StableDiffusionInpaintPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-inpainting\",\n",
    "    revision=\"fp16\",\n",
    "    torch_dtype=torch.float32\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_inpaint(img, comp_mask, device, vae):\n",
    "    x = transforms.ToTensor()(img).to(device)[None]\n",
    "    posterior = vae.encode(x).latent_dist\n",
    "    z = posterior.mode()\n",
    "    dec = vae.decode(z).sample[0]\n",
    "    inpainted = transforms.ToPILImage()(dec.clamp_(0, 1))\n",
    "    ret = Image.composite(inpainted, img, comp_mask)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 3 bowls, 1 broccoli, 17.1ms\n",
      "Speed: 1.5ms preprocess, 17.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70760fafb2cf4bbf962e0fa3ce6ed4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model = YOLO(\"yolov8m.pt\")\n",
    "outputdir = \"data/\"\n",
    "\n",
    "for i in range(3000):\n",
    "    row = captions.iloc[i]\n",
    "    im = Image.open(row.image).resize((512,512),PIL.Image.Resampling.BILINEAR)\n",
    "    w, h = im.size\n",
    "    caption = row.caption\n",
    "\n",
    "    results = model.predict(im, conf=0.5)[0]\n",
    "    \n",
    "    if len(results.boxes.data) > 0:\n",
    "        b = None\n",
    "        for box in results.boxes.data:\n",
    "            x, y, x_end, y_end, confidence, class_id = box\n",
    "            if (((y_end - y) * (x_end - x)) / 512**2) < 0.7:\n",
    "                break\n",
    "        \n",
    "    else:\n",
    "        x = round(random.random() * (w-minsize) + minsize/2)\n",
    "        y = round(random.random() * (h-minsize) + minsize/2)\n",
    "        cropw = round(random.random() * (w/2 - minsize)) + minsize\n",
    "        croph = round(random.random() * (h/2 - minsize)) + minsize\n",
    "        # print((x, y), (cropw, croph))\n",
    "        x = max(0, round(x - cropw/2))\n",
    "        y = max(0, round(y - croph/2))\n",
    "        x_end = min(w, x + cropw)\n",
    "        y_end = min(h, y + croph)\n",
    "\n",
    "    mask = PIL.Image.new(\"L\", im.size, 0)\n",
    "    draw = PIL.ImageDraw.Draw(mask)\n",
    "    draw.rectangle((x, y, x_end, y_end), fill=255)\n",
    "    blurred_mask = mask.filter(PIL.ImageFilter.GaussianBlur(radius=3))\n",
    "    actual_mask = blurred_mask.point( lambda p: 255 if p >= 255 else 0 )\n",
    "    # print((w, h), (x, y), (x_end, y_end))\n",
    "    black = PIL.Image.new(\"RGB\", im.size, 0)\n",
    "    image = PIL.Image.composite(black, im, actual_mask)\n",
    "\n",
    "    inpainted = pipe(prompt=caption, image=image, mask_image=actual_mask).images[0]\n",
    "\n",
    "\n",
    "    # fakeinpainted = fake_inpaint(original_image, blurred_mask, device=device, vae=pipe.vae)\n",
    "\n",
    "    inpainted.save(os.path.join(outputdir, str(i)+\".realfake.webp\"), lossless=True)\n",
    "\n",
    "    inpainted = Image.composite(inpainted, image, blurred_mask)\n",
    "    inpainted.save(os.path.join(outputdir, str(i)+\".inpainted.webp\"), lossless=True)\n",
    "\n",
    "    # fakeinpainted.save(os.path.join(outputdir, k+\".fakefake.webp\"), lossless=True)\n",
    "    blurred_mask.save(os.path.join(outputdir, str(i)+\".mask.webp\"), lossless=True)\n",
    "    im.save(os.path.join(outputdir, str(i)+\".original.webp\"), lossless=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
